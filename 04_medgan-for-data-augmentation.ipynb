{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "<br/>\n",
    "# Using `medGAN` to try to perform data augmentation on the MIMIC-III dataset with binary values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: [Sylvain Combettes](https://github.com/sylvaincom). <br/>\n",
    "Last update: Sep 3, 2019. Creation: Aug 29, 2019. <br/>\n",
    "My own medGAN repository (that is based on Edward Choi's work): [medgan](https://github.com/sylvaincom/medgan-tips). <br/>\n",
    "Edward Choi's original repository: [medgan](https://github.com/mp2893/medgan).\n",
    "\n",
    "Before reading this notebook, make sure that you have read my [medGAN repository](https://github.com/sylvaincom/medgan-tips)'s table of contents.\n",
    "\n",
    "> **Using `medGAN` to try to perform data augmentation** <br/> <br/>\n",
    "With `medGAN`, we want to generate (fake) realistic patient data, which can then enrich the initial training database.\n",
    "For example, my training dataset $A$ is not large enough (let it be 500 samples with 50 features) and we want to use `medGAN` to generate a new dataset $B$ of 1000 fake samples (with 50 features as well). By adding $B$ to $A$, we get a new training dataset $C$ that has 1500 patients. We can hope that $C$ helps algorithms (any one of them) make better predictions than $A$. <br/> <br/>\n",
    "I asked Edward Choi what he thought about using the generative model GANs for data augmentation. Trying to generate fake realistic patients with `medGAN` from a dataset of 500 samples with 250 variables seems suboptimal: there seems to be too many variables and not enough samples. There is no definite number as to how many variables we need to delete: it depends on the variance of each variable and the correlation between variables. For example, if there is a variable named \"gender\" and all 500 samples are from men (thus low variance), then it would be very easy for `medGAN` to replicate that variable (by putting men as gender for each generated sample).\n",
    "\n",
    "We will use the MIMIC-III dataset and process it so that we only have binary values.\n",
    "\n",
    "From now on, whenever we refer to \"input\" or \"output\", we refer to the input and output of medgan.py (unless specified otherwise). \"input\" is the original real-life dataset and \"output\" is the fake realistic generated dataset.\n",
    "\n",
    "Warning: the computing time is _very_ long."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Table of contents\n",
    "\n",
    "- [Loading the data](#load)\n",
    "- [Predicting the column of index `10` (called `target`) with (only) the original real-life dataset](#pred1)\n",
    "- [Predicting the column of index `10` (called `target`) with (only) the (fake) generated dataset](#pred2)\n",
    "- [Predicting the column of index `10` (called `target`) with data augmentation](#pred3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from time import process_time\n",
    "import datetime\n",
    "\n",
    "from sklearn import datasets, model_selection, linear_model, neighbors, neural_network, naive_bayes\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Loading the data <a name=\"load\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the real-life original dataset\n",
    "\n",
    "We refer to the real-life original dataset as `df_input`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_array = pickle.load(open('training-data.matrix', 'rb'))\n",
    "df_input = pd.DataFrame(input_data_array)\n",
    "print('The shape of the real-life original dataset is :', df_input.shape)\n",
    "df_input.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the (fake) generated dataset\n",
    "\n",
    "We refer to the (fake) generated dataset as `df_output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.load('gen-samples.npy')\n",
    "df_output = pd.DataFrame(output).round(0)\n",
    "print('The shape of the (fake) generated dataset is :', df_output.shape)\n",
    "df_output.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the feature we are going to try to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which feature are we going to try to predict? For example, we want one with not only zeros (si it is harder to predict it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_input.sum()/df_input.shape[0], 'o')\n",
    "plt.xlabel('Index of feature')\n",
    "plt.ylabel('Proportion of 1s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 10\n",
    "print('Approx. proportion of 1s of target :', round(df_input[target].sum()/df_input.shape[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Predicting the column of index `10` (called `target`) with (only) the original real-life dataset <a name=\"pred1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_input\n",
    "X_dataset = df.loc[:, df.columns != target].values\n",
    "y_dataset = np.ravel(df.loc[:, df.columns == target].values)\n",
    "print(X_dataset.shape, y_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking some models according to their score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def score_and_time(model, X_dataset, y_dataset, cv):\n",
    "    \"\"\"\n",
    "    When there are no hyper-parameters.\n",
    "    This function returns a list with the scores and processing times of model.\n",
    "    The scores are calculated with cross_val_score (with K-Fold equal to cv).\n",
    "    \"\"\"\n",
    "    t_start = process_time()\n",
    "    scores = model_selection.cross_val_score(model, X_dataset, y_dataset, cv=cv)\n",
    "    t_stop = process_time()\n",
    "    part_l = [round(scores.mean(), 3), round(scores.std()*2, 3), str(datetime.timedelta(seconds=t_stop-t_start))]\n",
    "    return part_l\n",
    "\n",
    "def score_and_time_hyp(model, parameters, X_dataset, y_dataset, cv):\n",
    "    \"\"\"\n",
    "    When there are hyper-parameters.\n",
    "    This function returns a list with the scores and processing times of model.\n",
    "    The scores are calculated with RandomizedSearchCV (with K-Fold equal to cv).\n",
    "    \"\"\"\n",
    "    t_start = process_time()\n",
    "    clf_grid = RandomizedSearchCV(model, parameters, cv=cv, n_jobs=-1)  \n",
    "    clf_grid.fit(X_dataset, y_dataset)\n",
    "    scores = clf_grid.best_score_ # mean cross-validated score of the best_estimator\n",
    "    t_stop = process_time()\n",
    "    part_l = [round(scores.mean(), 3), '-', str(datetime.timedelta(seconds=t_stop-t_start))]\n",
    "    return part_l\n",
    "\n",
    "def ml_benchmark(X_dataset, y_dataset, cv):\n",
    "    \"\"\"\n",
    "    This function returns a pandas dataframe with the scores and processing times of some classic machine learning models\n",
    "    applied to X_dataset and y_dataset.\n",
    "    The scores are calculated with cross_val_score (with K-Fold equal to cv).\n",
    "    If there are hyper-parameters, there are computed with RandomizedSearchCV.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('The shape of X_dataset is :', X_dataset.shape)\n",
    "    print('The shape of y_dataset is :', y_dataset.shape)\n",
    "     \n",
    "    rows_name = [\"Ridge\", \"Lasso\", \"Logistic Regression\", \"Nearest Neighbors\", \"Naive Bayes\",\n",
    "                  \"Perceptron\", \"Random Forest\", \"Multi-Layer Perceptron\"]\n",
    "    \n",
    "    columns_name = ['Approx. mean of scores', 'Approx. variance of scores', 'Processing time']\n",
    "    \n",
    "    l = []\n",
    "        \n",
    "    model = Ridge()\n",
    "    l.append(score_and_time(model, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    model = Lasso()\n",
    "    l.append(score_and_time(model, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    model = linear_model.LogisticRegression()\n",
    "    parameters = {'solver': ['lbfgs','liblinear','sag','saga'], 'multi_class': ['auto'],\n",
    "                 'warm_start': [True, False], 'C': [0.01,0.1,1,10,100]}\n",
    "    l.append(score_and_time_hyp(model, parameters, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    model = neighbors.KNeighborsClassifier()\n",
    "    parameters = {'n_neighbors': [1,2,3,5,8,10,20], 'algorithm': ['ball_tree', 'kd_tree', 'brute']}\n",
    "    l.append(score_and_time_hyp(model, parameters, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    model = naive_bayes.GaussianNB()\n",
    "    l.append(score_and_time(model, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    model = linear_model.Perceptron()\n",
    "    l.append(score_and_time(model, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    parameters = {'n_estimators': [1000], 'max_depth': [1,10,25,50], \"bootstrap\": [True, False],\n",
    "                  \"max_features\": [1, 3, 10], \"min_samples_split\": [2, 3, 10],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"], 'random_state': [0]}\n",
    "    l.append(score_and_time_hyp(model, parameters, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    model = neural_network.MLPClassifier()\n",
    "    parameters = {'solver': ['lbfgs'], 'max_iter': [1,500,1000,1500,2000], 'alpha': 10.0**-np.arange(1,5),\n",
    "                  'hidden_layer_sizes': np.arange(1,15,2),'activation': ['relu','tanh']}\n",
    "    l.append(score_and_time_hyp(model, parameters, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    out = pd.DataFrame(l, index = rows_name, columns = columns_name)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_benchmark = ml_benchmark(X_dataset, y_dataset, 5)\n",
    "input_benchmark.sort_values(by=['Approx. mean of scores'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export the results into a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_benchmark.to_csv('input_benchmark.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Predicting the column of index `10` (called `target`) with (only) the (fake) generated dataset <a name=\"pred2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_output\n",
    "X_dataset = df.loc[:, df.columns != target].values\n",
    "y_dataset = np.ravel(df.loc[:, df.columns == target].values)\n",
    "print(X_dataset.shape, y_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking some models according to their score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_benchmark = ml_benchmark(X_dataset, y_dataset, 5)\n",
    "output_benchmark.sort_values(by=['Approx. mean of scores'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export the results into a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_benchmark.to_csv('output_benchmark.csv', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Predicting the column of index `10` (called `target`) with data augmentation <a name=\"pred3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the data\n",
    "\n",
    "We concatenate the real-life dataset `df_input` and the (fake) generated dataset `df_output` into `df_aug`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug = df_input.append(df_output)\n",
    "print(df_aug.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_aug\n",
    "X_dataset = df.loc[:, df.columns != target].values\n",
    "y_dataset = np.ravel(df.loc[:, df.columns == target].values)\n",
    "print(X_dataset.shape, y_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking some models according to their score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_benchmark = ml_benchmark(X_dataset, y_dataset, 5)\n",
    "aug_benchmark.sort_values(by=['Approx. mean of scores'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We export the results into a csv file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_benchmark.to_csv('aug_benchmark', sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Comparison: does data augmentation help boost the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xaxis = input_benchmark['Approx. mean of scores'].values\n",
    "yaxis = aug_benchmark['Approx. mean of scores'].values\n",
    "\n",
    "start = min(np.min(xaxis), np.min(yaxis))\n",
    "stop = max(np.max(xaxis), np.max(yaxis))\n",
    "p = len(xaxis)\n",
    "X = np.linspace(start, stop, num=p+1)\n",
    "\n",
    "plt.plot(xaxis, yaxis, 'ok', X, X, '-g');\n",
    "\n",
    "plt.legend(['Approx. mean of scores', 'Equal approx. mean of scores'])\n",
    "plt.title('Dimension-wise probability performance of medGAN')\n",
    "plt.xlabel('For the real dataset')\n",
    "plt.ylabel('For the (fake) generated dataset')\n",
    "plt.savefig('comparison.png') # to save the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Back to [top](#top)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
