{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"top\"></a>\n",
    "<br/>\n",
    "# Using `medGAN` to perform data augmentation on the MIMIC-III dataset of shape `(1000, 100)` with binary values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: [Sylvain Combettes](https://github.com/sylvaincom). <br/>\n",
    "Last update: Sep 4, 2019. Creation: Aug 29, 2019. <br/>\n",
    "My own medGAN repository (that is based on Edward Choi's work): [medgan](https://github.com/sylvaincom/medgan-tips). <br/>\n",
    "Edward Choi's original repository: [medgan](https://github.com/mp2893/medgan).\n",
    "\n",
    "Before reading this notebook, make sure that you have read my [medGAN repository](https://github.com/sylvaincom/medgan-tips)'s table of contents.\n",
    "\n",
    "> **Using `medGAN` to try to perform data augmentation** <br/> <br/>\n",
    "With `medGAN`, we want to generate (fake) realistic patient data, which can then enrich the initial training database.\n",
    "For example, my training dataset $A$ is not large enough (let it be 500 samples with 50 features) and we want to use `medGAN` to generate a new dataset $B$ of 1000 fake samples (with 50 features as well). By adding $B$ to $A$, we get a new training dataset $C$ that has 1500 patients. We can hope that $C$ helps algorithms (any one of them) make better predictions than $A$. <br/> <br/>\n",
    "I asked Edward Choi what he thought about using the generative model GANs for data augmentation. Trying to generate fake realistic patients with `medGAN` from a dataset of 500 samples with 250 variables seems suboptimal: there seems to be too many variables and not enough samples. There is no definite number as to how many variables we need to delete: it depends on the variance of each variable and the correlation between variables. For example, if there is a variable named \"gender\" and all 500 samples are from men (thus low variance), then it would be very easy for `medGAN` to replicate that variable (by putting men as gender for each generated sample).\n",
    "\n",
    "We will use the MIMIC-III dataset and process it so that we only have binary values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Table of contents\n",
    "\n",
    "- [1) Loading the data](#load)\n",
    "- [2) Predicting the column of index `5` (called `target`) with (only) the original real-life dataset](#input)\n",
    "- [3) Predicting the column of index `5` (called `target`) with (only) the fictitious generated dataset](#output)\n",
    "- [4) Predicting the column of index `5` (called `target`) with data augmentation](#aug)\n",
    "- [5) Can training on the augmented dataset help improve the prediction score with a real-life test set?](#aug2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from time import process_time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "from sklearn import datasets, model_selection, linear_model, neighbors, neural_network, naive_bayes\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"load\"></a>\n",
    "# 1) Loading the data\n",
    "\n",
    "## 1.1) Loading the real-life original dataset\n",
    "\n",
    "We refer to the real-life original dataset as `df_real`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_data_array = pickle.load(open('training-data-small.matrix', 'rb'))\n",
    "df_real = pd.DataFrame(real_data_array)\n",
    "print('The shape of the real-life original dataset is :', df_real.shape)\n",
    "df_real.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2) Loading the fictitious generated dataset\n",
    "\n",
    "We refer to the fictitious generated dataset as `df_fict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fict = np.load('gen-samples.npy')\n",
    "df_fict = pd.DataFrame(fict).round(0)\n",
    "print('The shape of the fictitious generated dataset is :', df_fict.shape)\n",
    "df_fict.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3) Choosing the feature we are going to try to predict\n",
    "\n",
    "Which feature are we going to try to predict? For example, we want one with not only zeros (si it is harder to predict it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_real.sum()/df_real.shape[0], 'o')\n",
    "plt.xlabel('Index of feature')\n",
    "plt.ylabel('Proportion of 1s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = df_real.sum().idxmax(axis=0) # take 61 also\n",
    "print('Approx. proportion of 1s of target ( of index', target, '):',\n",
    "      round(df_real[target].sum()/df_real.shape[0], 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"input\"></a>\n",
    "# 2) Predicting the column of index `5` (called `target`) with (only) the original real-life dataset\n",
    "\n",
    "## 2.1) Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_real\n",
    "X_dataset = df.loc[:, df.columns != target].values\n",
    "y_dataset = np.ravel(df.loc[:, df.columns == target].values)\n",
    "print(X_dataset.shape, y_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2) Benchmarking some models according to their score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_and_time(model, X_dataset, y_dataset, cv):\n",
    "    \"\"\"\n",
    "    When there are no hyper-parameters.\n",
    "    This function returns a list with the scores and processing times of model.\n",
    "    The scores are calculated with cross_val_score (with K-Fold equal to cv).\n",
    "    \"\"\"\n",
    "    t_start = process_time()\n",
    "    scores = model_selection.cross_val_score(model, X_dataset, y_dataset, cv=cv)\n",
    "    t_stop = process_time()\n",
    "    part_l = [round(scores.mean(), 3), round(scores.std()*2, 3), str(datetime.timedelta(seconds=t_stop-t_start))]\n",
    "    return part_l\n",
    "\n",
    "def score_and_time_hyp(model, parameters, X_dataset, y_dataset, cv):\n",
    "    \"\"\"\n",
    "    When there are hyper-parameters.\n",
    "    This function returns a list with the scores and processing times of model.\n",
    "    The scores are calculated with RandomizedSearchCV (with K-Fold equal to cv) (we use a random seed).\n",
    "    \"\"\"\n",
    "    t_start = process_time()\n",
    "    clf_grid = RandomizedSearchCV(model, parameters, cv=cv, random_state=1, n_jobs=-1)  \n",
    "    clf_grid.fit(X_dataset, y_dataset)\n",
    "    scores = clf_grid.best_score_ # mean cross-validated score of the best_estimator\n",
    "    t_stop = process_time()\n",
    "    part_l = [round(scores.mean(), 3), '-', str(datetime.timedelta(seconds=t_stop-t_start))]\n",
    "    return part_l\n",
    "\n",
    "def ml_benchmark(X_dataset, y_dataset, cv):\n",
    "    \"\"\"\n",
    "    This function returns a pandas dataframe with the scores and processing times of some classic machine learning models\n",
    "    applied to X_dataset and y_dataset.\n",
    "    The scores are calculated with cross_val_score (with K-Fold equal to cv).\n",
    "    If there are hyper-parameters, there are computed with RandomizedSearchCV.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('The shape of X_dataset is :', X_dataset.shape)\n",
    "    print('The shape of y_dataset is :', y_dataset.shape)\n",
    "     \n",
    "    rows_name = [\"Ridge\", \"Lasso\", \"Logistic Regression\", \"Nearest Neighbors\", \"Naive Bayes\",\n",
    "                  \"Perceptron\", \"Random Forest\", \"Multi-Layer Perceptron\"]\n",
    "    \n",
    "    columns_name = ['Approx. mean of scores', 'Approx. variance of scores', 'Processing time']\n",
    "    \n",
    "    l = []\n",
    "        \n",
    "    model = Ridge()\n",
    "    l.append(score_and_time(model, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    model = Lasso()\n",
    "    l.append(score_and_time(model, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    model = linear_model.LogisticRegression()\n",
    "    parameters = {'solver': ['lbfgs','liblinear','sag','saga'], 'multi_class': ['auto'],\n",
    "                 'warm_start': [True, False], 'C': [0.01,0.1,1,10,100]}\n",
    "    l.append(score_and_time_hyp(model, parameters, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    model = neighbors.KNeighborsClassifier()\n",
    "    parameters = {'n_neighbors': [1,2,3,5,8,10,20], 'algorithm': ['ball_tree', 'kd_tree', 'brute']}\n",
    "    l.append(score_and_time_hyp(model, parameters, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    model = naive_bayes.GaussianNB()\n",
    "    l.append(score_and_time(model, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    model = linear_model.Perceptron()\n",
    "    l.append(score_and_time(model, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    parameters = {'n_estimators': [1000], 'max_depth': [1,10,25,50], \"bootstrap\": [True, False],\n",
    "                  \"max_features\": [1, 3, 10], \"min_samples_split\": [2, 3, 10],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"], 'random_state': [0]}\n",
    "    l.append(score_and_time_hyp(model, parameters, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    model = neural_network.MLPClassifier()\n",
    "    parameters = {'solver': ['lbfgs'], 'max_iter': [1,500,1000,1500,2000], 'alpha': 10.0**-np.arange(1,5),\n",
    "                  'hidden_layer_sizes': np.arange(1,15,2),'activation': ['relu','tanh']}\n",
    "    l.append(score_and_time_hyp(model, parameters, X_dataset, y_dataset, cv))\n",
    "    \n",
    "    out = pd.DataFrame(l, index = rows_name, columns = columns_name)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_benchmark = ml_benchmark(X_dataset, y_dataset, 5)\n",
    "real_benchmark.to_csv('real_benchmark.csv', sep=';') # saving the results\n",
    "\n",
    "real_benchmark_sorted = real_benchmark.sort_values(by=['Approx. mean of scores'], ascending=False)\n",
    "real_benchmark_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"output\"></a>\n",
    "# 3) Predicting the column of index `5` (called `target`) with (only) the fictitious generated dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1) Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_fict\n",
    "X_dataset = df.loc[:, df.columns != target].values\n",
    "y_dataset = np.ravel(df.loc[:, df.columns == target].values)\n",
    "print(X_dataset.shape, y_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2) Benchmarking some models according to their score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fict_benchmark = ml_benchmark(X_dataset, y_dataset, 5)\n",
    "fict_benchmark.to_csv('fict_benchmark.csv', sep=';') # saving the results\n",
    "\n",
    "fict_benchmark_sorted = fict_benchmark.sort_values(by=['Approx. mean of scores'], ascending=False)\n",
    "fict_benchmark_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"aug\"></a>\n",
    "# 4) Predicting the column of index `5` (called `target`) with data augmentation\n",
    "\n",
    "## 4.1) Preparing the data\n",
    "\n",
    "We concatenate the real-life dataset `df_real` and the fictitious generated dataset `df_fict` into an augmented dataset `df_aug`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_aug = df_real.append(df_fict)\n",
    "\n",
    "X_dataset = df_aug.loc[:, df_aug.columns != target].values\n",
    "y_dataset = np.ravel(df_aug.loc[:, df_aug.columns == target].values)\n",
    "print(X_dataset.shape, y_dataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Benchmarking some models according to their score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_benchmark = ml_benchmark(X_dataset, y_dataset, 5)\n",
    "aug_benchmark.to_csv('aug_benchmark.csv', sep=';') # saving the results\n",
    "\n",
    "aug_benchmark_sorted = aug_benchmark.sort_values(by=['Approx. mean of scores'], ascending=False)\n",
    "aug_benchmark_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) Comparison: can data augmentation boost the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xaxis = real_benchmark['Approx. mean of scores'].values\n",
    "yaxis = aug_benchmark['Approx. mean of scores'].values\n",
    "\n",
    "start = min(np.min(xaxis), np.min(yaxis))\n",
    "stop = max(np.max(xaxis), np.max(yaxis))\n",
    "p = len(xaxis)\n",
    "X = np.linspace(start, stop, num=p+1)\n",
    "\n",
    "plt.plot(xaxis, yaxis, 'ok', X, X, '-g');\n",
    "\n",
    "plt.legend(['Approx. mean of scores', 'Equal approx. mean of scores'])\n",
    "plt.title('Boosting the prediction score with data augmentation')\n",
    "plt.xlabel('For the real dataset')\n",
    "plt.ylabel('For the fictitious generated dataset')\n",
    "plt.savefig('comparison_small.png', dpi=120) # to save the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the $x$-axis and the $y$-axis are ordered.\n",
    "\n",
    "The diagonal green line indicates where the real and the (fake) realistic generated data show identical performance for a given machine learning model. Based on the previous graph, we can say that `medGAN` can perform data augmentation and boost the prediction score.\n",
    "\n",
    "By what percentage did we increase of prediction score of our best machine learning models with data augmentation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_real = real_benchmark['Approx. mean of scores'].values\n",
    "score_aug = aug_benchmark['Approx. mean of scores'].values\n",
    "score_inc = (score_aug-score_real)/score_real*100\n",
    "df_score_inc = round(pd.DataFrame(score_inc, index=real_benchmark.index.values, columns=['Prediction score increase (%)']), 2)\n",
    "df_score_inc.to_csv('scores_increase_small.csv', sep=';')\n",
    "df_score_inc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practise, we would choose the machine learning algorithm with the best score on the real-life dataset. Then, we use this ML algorithm on the augmented dataset to train our model then do the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_ml = real_benchmark_sorted.index.values[0]\n",
    "print('The best machine learning model on the original real-life dataset is :', best_ml)\n",
    "best_ml_aug = aug_benchmark_sorted.index.values[0]\n",
    "print('The best machine learning model on the augmented dataset is :', best_ml_aug)\n",
    "\n",
    "percentage = df_score_inc.loc[[best_ml], ['Prediction score increase (%)']].values[0][0]\n",
    "print('We increased the prediction score of', best_ml, 'by approx.', percentage, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With data augmentation, we have an increase in score: up to almost 8%! However, we kind of cheated because half of the values of `target` in the augmented dataset are fictitious generated values. We now try to avoid this problem by taking only real-life values for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"aug2\"></a>\n",
    "# 5) Can training on the augmented dataset help improve the prediction score with a real-life test set?\n",
    "\n",
    "## 5.1) Without data augmentation\n",
    "\n",
    "#### 5.1.1) Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_real.loc[:, df_real.columns != target].values\n",
    "y_train = np.ravel(df_real.loc[:, df_real.columns == target].values)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.1.b) For the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import the full real-life test:\n",
    "real_data_array_full = pickle.load(open('training-data.matrix', 'rb'))\n",
    "df_real_full = pd.DataFrame(real_data_array_full)\n",
    "\n",
    "# We only keep the 100st columns:\n",
    "col = list(df_real_full.columns)[100:1071]\n",
    "df_real_full.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# We only select some rows for the real-life dataset to be the test set\n",
    "n,p = df_aug.shape\n",
    "print('The shape of the augmented training set is', (n,p))\n",
    "row_start = 1000\n",
    "df_test = df_real_full[row_start:row_start+n//8]\n",
    "print('The shape of our test set is', df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.loc[:, df_test.columns != target].values\n",
    "y_test = np.ravel(df_test.loc[:, df_test.columns == target].values)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2) Benchmarking some models according to their score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(model, X_train, y_train, X_test, y_test):\n",
    "    t_start = process_time()\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    t_stop = process_time()\n",
    "    return [round(score, 3), str(datetime.timedelta(seconds=t_stop-t_start))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ml_benchmark(X_train, y_train, X_test, y_test, cv):\n",
    "    \"\"\"\n",
    "    This function returns a pandas dataframe with the scores and processing times of some classic\n",
    "    machine learning models.\n",
    "    If there are hyper-parameters, there are computed with RandomizedSearchCV.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('The shape of X_train is :', X_train.shape)\n",
    "    print('The shape of y_train is :', y_train.shape)\n",
    "     \n",
    "    rows_name = [\"Ridge\", \"Lasso\", \"Logistic Regression\", \"Nearest Neighbors\", \"Naive Bayes\",\n",
    "                  \"Perceptron\", \"Random Forest\", \"Multi-Layer Perceptron\"]\n",
    "    \n",
    "    columns_name = ['Approx. score', 'Processing time']\n",
    "    \n",
    "    l = []\n",
    "        \n",
    "    model = Ridge()\n",
    "    l.append(score(model, X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    model = Lasso()\n",
    "    l.append(score(model, X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    model = linear_model.LogisticRegression()\n",
    "    parameters = {'solver': ['lbfgs','liblinear','sag','saga'], 'multi_class': ['auto'],\n",
    "                 'warm_start': [True, False], 'C': [0.01,0.1,1,10,100]}\n",
    "    clf_grid = RandomizedSearchCV(model, parameters, cv=cv, random_state=1, n_jobs=-1)  \n",
    "    clf_grid.fit(X_train, y_train)\n",
    "    final_model = linear_model.LogisticRegression(**clf_grid.best_params_)\n",
    "    l.append(score(final_model, X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    model = neighbors.KNeighborsClassifier()\n",
    "    parameters = {'n_neighbors': [1,2,3,5,8,10,20], 'algorithm': ['ball_tree', 'kd_tree', 'brute']}\n",
    "    clf_grid = RandomizedSearchCV(model, parameters, cv=cv, random_state=1, n_jobs=-1)  \n",
    "    clf_grid.fit(X_train, y_train)\n",
    "    final_model = neighbors.KNeighborsClassifier(**clf_grid.best_params_)\n",
    "    l.append(score(final_model, X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    model = naive_bayes.GaussianNB()\n",
    "    l.append(score(model, X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    model = linear_model.Perceptron()\n",
    "    l.append(score(model, X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    model = RandomForestClassifier()\n",
    "    parameters = {'n_estimators': [1000], 'max_depth': [1,10,25,50], \"bootstrap\": [True, False],\n",
    "                  \"max_features\": [1, 3, 10], \"min_samples_split\": [2, 3, 10],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"], 'random_state': [0]}\n",
    "    clf_grid = RandomizedSearchCV(model, parameters, cv=cv, random_state=1, n_jobs=-1)  \n",
    "    clf_grid.fit(X_train, y_train)\n",
    "    final_model = RandomForestClassifier(**clf_grid.best_params_)\n",
    "    l.append(score(final_model, X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    model = neural_network.MLPClassifier()\n",
    "    parameters = {'solver': ['lbfgs'], 'max_iter': [1,500,1000,1500,2000], 'alpha': 10.0**-np.arange(1,5),\n",
    "                  'hidden_layer_sizes': np.arange(1,15,2),'activation': ['relu','tanh']}\n",
    "    clf_grid = RandomizedSearchCV(model, parameters, cv=cv, random_state=1, n_jobs=-1)  \n",
    "    clf_grid.fit(X_train, y_train)\n",
    "    final_model = neural_network.MLPClassifier(**clf_grid.best_params_)\n",
    "    l.append(score(final_model, X_train, y_train, X_test, y_test))\n",
    "    \n",
    "    out = pd.DataFrame(l, index = rows_name, columns = columns_name)\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_benchmark_2 = ml_benchmark(X_train, y_train, X_test, y_test, 5)\n",
    "real_benchmark_2.to_csv('real_benchmark_2.csv', sep=';') # saving the results\n",
    "\n",
    "real_benchmark_2_sorted = real_benchmark_2.sort_values(by=['Approx. score'], ascending=False)\n",
    "real_benchmark_2_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2) With data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_aug.loc[:, df_aug.columns != target].values\n",
    "y_train = np.ravel(df_aug.loc[:, df_aug.columns == target].values)\n",
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_benchmark_2 = ml_benchmark(X_train, y_train, X_test, y_test, 5)\n",
    "aug_benchmark_2.to_csv('aug_benchmark_2.csv', sep=';') # saving the results\n",
    "\n",
    "aug_benchmark_2_sorted = aug_benchmark_2.sort_values(by=['Approx. score'], ascending=False)\n",
    "aug_benchmark_2_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3) Comparison: can data augmentation boost the score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xaxis = real_benchmark_2['Approx. score'].values\n",
    "yaxis = aug_benchmark_2['Approx. score'].values\n",
    "\n",
    "start = min(np.min(xaxis), np.min(yaxis))\n",
    "stop = max(np.max(xaxis), np.max(yaxis))\n",
    "p = len(xaxis)\n",
    "X = np.linspace(start, stop, num=p+1)\n",
    "\n",
    "plt.plot(xaxis, yaxis, 'ok', X, X, '-g');\n",
    "\n",
    "plt.legend(['Approx. mean of scores', 'Equal approx. mean of scores'])\n",
    "plt.title('Boosting the prediction score with data augmentation')\n",
    "plt.xlabel('For the real dataset')\n",
    "plt.ylabel('For the fictitious generated dataset')\n",
    "plt.savefig('comparison_small_2.png', dpi=120) # to save the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_real_2 = real_benchmark_2['Approx. score'].values\n",
    "score_aug_2 = aug_benchmark_2['Approx. score'].values\n",
    "score_inc_2 = (score_aug_2-score_real_2)/score_real_2*100\n",
    "df_score_inc_2 = round(pd.DataFrame(score_inc_2, index=real_benchmark.index.values, columns=['Prediction score increase (%)']), 2)\n",
    "df_score_inc_2.to_csv('scores_increase_small_2.csv', sep=';')\n",
    "df_score_inc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ml = real_benchmark_2_sorted.index.values[0]\n",
    "print('The best machine learning model on the original real-life dataset is :', best_ml)\n",
    "best_ml_aug = aug_benchmark_2_sorted.index.values[0]\n",
    "print('The best machine learning model on the augmented dataset is :', best_ml_aug)\n",
    "\n",
    "percentage = df_score_inc_2.loc[[best_ml], ['Prediction score increase (%)']].values[0][0]\n",
    "print('We increased the prediction score of', best_ml, 'by approx.', percentage, '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Back to [top](#top)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
